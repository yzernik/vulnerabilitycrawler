   net: add limit for socket backlog
   
   We got system OOM while running some UDP netperf testing on the loopback
   device. The case is multiple senders sent stream UDP packets to a single
   receiver via loopback on local host. Of course, the receiver is not able
   to handle all the packets in time. But we surprisingly found that these
   packets were not discarded due to the receiver's sk->sk_rcvbuf limit.
   Instead, they are kept queuing to sk->sk_backlog and finally ate up all
   the memory. We believe this is a secure hole that a none privileged user
   can crash the system.
   
   The root cause for this problem is, when the receiver is doing
   __release_sock() (i.e. after userspace recv, kernel udp_recvmsg ->
   skb_free_datagram_locked -> release_sock), it moves skbs from backlog to
   sk_receive_queue with the softirq enabled. In the above case, multiple
   busy senders will almost make it an endless loop. The skbs in the
   backlog end up eat all the system memory.
   
   The issue is not only for UDP. Any protocols using socket backlog is
   potentially affected. The patch adds limit for socket backlog so that
   the backlog size cannot be expanded endlessly.
   
   Reported-by: Alex Shi <alex.shi@intel.com>
   Cc: David Miller <davem@davemloft.net>
   Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
   Cc: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru
   Cc: "Pekka Savola (ipv6)" <pekkas@netcore.fi>
   Cc: Patrick McHardy <kaber@trash.net>
   Cc: Vlad Yasevich <vladislav.yasevich@hp.com>
   Cc: Sridhar Samudrala <sri@us.ibm.com>
   Cc: Jon Maloy <jon.maloy@ericsson.com>
   Cc: Allan Stephens <allan.stephens@windriver.com>
   Cc: Andrew Hendry <andrew.hendry@gmail.com>
   Signed-off-by: Zhu Yi <yi.zhu@intel.com>
   Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
   Acked-by: Arnaldo Carvalho de Melo <acme@redhat.com>
   Signed-off-by: David S. Miller <davem@davemloft.net>
	struct {
		struct sk_buff *head;
		struct sk_buff *tail;
	} sk_backlog;
	wait_queue_head_t	*sk_sleep;
	struct dst_entry	*sk_dst_cache;
	return sk->sk_wmem_queued < sk->sk_sndbuf;
}

/* The per-socket spinlock must be held here. */
static inline void sk_add_backlog(struct sock *sk, struct sk_buff *skb)
{
	if (!sk->sk_backlog.tail) {
	skb->next = NULL;
}

static inline int sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)
{
	return sk->sk_backlog_rcv(sk, skb);
		rc = sk_backlog_rcv(sk, skb);

		mutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);
	} else
		sk_add_backlog(sk, skb);
	bh_unlock_sock(sk);
out:
	sock_put(sk);
		sock_lock_init(newsk);
		bh_lock_sock(newsk);
		newsk->sk_backlog.head	= newsk->sk_backlog.tail = NULL;

		atomic_set(&newsk->sk_rmem_alloc, 0);
		/*

		bh_lock_sock(sk);
	} while ((skb = sk->sk_backlog.head) != NULL);
}

/**
	sk->sk_allocation	=	GFP_KERNEL;
	sk->sk_rcvbuf		=	sysctl_rmem_default;
	sk->sk_sndbuf		=	sysctl_wmem_default;
	sk->sk_state		=	TCP_CLOSE;
	sk_set_socket(sk, sock);

